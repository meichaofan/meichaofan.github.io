<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[利用docker搭建samba目录共享]]></title>
    <url>%2F2019%2F04%2F22%2Fdocker-run-samba%2F</url>
    <content type="text"><![CDATA[1.下载samba镜像1docker pull dperson/samba 2.启动镜像，具体配置看文档，但重要的配置是一下的注释12345678docker run --name samba \-it -p 139:139 -p 445:445 \-v /root:/root \-v /etc/passwd:/etc/passwd \-v /etc/group:/etc/group \-d dperson/samba \-u &quot;meichaofan;huanhuan0921&quot; \-s &quot;meichaofan home;/root;yes;no;no;all;none&quot; 3.替换samba的启动用户，与权限相关1docker exec -it samba sed -i &apos;s/force user = smbuser/force user = meichaofan/g&apos; /etc/samba/smb.conf 3.替换samba的启动组，与权限相关1docker exec -it samba sed -i &apos;s/force group = users/force group = meichaofan/g&apos; /etc/samba/smb.conf 4.重启samba1docker restart samba]]></content>
      <tags>
        <tag>docker</tag>
        <tag>samba</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 存储驱动之 overlay]]></title>
    <url>%2F2019%2F04%2F21%2Fdocker-overlay%2F</url>
    <content type="text"></content>
      <tags>
        <tag>docker</tag>
        <tag>overlayFs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 存储驱动之 overlay2]]></title>
    <url>%2F2019%2F04%2F21%2Fdocker-overlay2%2F</url>
    <content type="text"><![CDATA[overlay2中镜像和容器的磁盘结构docker pull ubuntu:14.04下载了包含4层的镜像，12345678[root@www ~]# docker pull ubuntu:14.0414.04: Pulling from library/ubuntue082d4499130: Pull complete 371450624c9e: Pull complete c8a555b3a57c: Pull complete 1456d810d42e: Pull complete Digest: sha256:6612de24437f6f01d6a2988ed9a36b3603df06e8d2c0493678f3ee696bc4bb2dStatus: Downloaded newer image for ubuntu:14.04 可以在/var/lib/docker/overlay2中看到，有5个目录。1234567[root@www overlay2]# ll -htotal 28Kdrwx------ 4 root root 4.0K Apr 21 00:32 0734cd8060d194cf3db93162b421e4f245151920f0b9acda5313ec9f671bc5ccdrwx------ 3 root root 4.0K Apr 21 00:32 153f0c95b638414d41bb07b9d45243a0219719e468dd9dcb097b80f837b4d8b3drwx------ 4 root root 4.0K Apr 21 00:32 658b3e84761843f58ecaf82e1f987bf32d498b7bd54a9cf40bf6bf635fff8ac3drwx------ 4 root root 4.0K Apr 21 00:32 9ff7c96e688b25a7353c83904d791eae357c2e16315ecbe602009678965ce761drwx------ 2 root root 4.0K Apr 21 00:33 l l目录的内容其中l目录中包含了很多软连接，使用短名称指向了其它层。短名称用于避免mount参数时达到页面大小的限制。123456[root@www overlay2]# ll l/total 24lrwxrwxrwx 1 root root 72 Apr 21 00:32 2AQ2F7X67IL4TATXIIYO62CM67 -&gt; ../153f0c95b638414d41bb07b9d45243a0219719e468dd9dcb097b80f837b4d8b3/difflrwxrwxrwx 1 root root 72 Apr 21 00:32 HMOMCX3OAMM4K6KUZORQ7NTIG4 -&gt; ../9ff7c96e688b25a7353c83904d791eae357c2e16315ecbe602009678965ce761/difflrwxrwxrwx 1 root root 72 Apr 21 00:32 JZPHZPG7ZLX7GTKCFVHQZDEF43 -&gt; ../658b3e84761843f58ecaf82e1f987bf32d498b7bd54a9cf40bf6bf635fff8ac3/difflrwxrwxrwx 1 root root 72 Apr 21 00:32 OJ5PMEVISSBQ4X4N2U66YEE6IH -&gt; ../0734cd8060d194cf3db93162b421e4f245151920f0b9acda5313ec9f671bc5cc/diff mount 查看容器/镜像的层次关系现在起一个容器，它会在rootfs层上，加上init层（环境相关）和容器层（读写）1docker run -it ubuntu:14.04 /bin/bash 查看mount相关信息，可以看出层级关系123[root@www ~]# mount -l | grep overlay2/dev/vda1 on /var/lib/docker/overlay2 type ext3 (rw,relatime,data=ordered)overlay on /var/lib/docker/overlay2/bda1eaf86f4c4a500aac2418bc45956531e81de6fff829b67452174c891f5f15/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/MXQORLOJXLIRFVZ3R26TNWWURM:/var/lib/docker/overlay2/l/HMOMCX3OAMM4K6KUZORQ7NTIG4:/var/lib/docker/overlay2/l/OJ5PMEVISSBQ4X4N2U66YEE6IH:/var/lib/docker/overlay2/l/JZPHZPG7ZLX7GTKCFVHQZDEF43:/var/lib/docker/overlay2/l/2AQ2F7X67IL4TATXIIYO62CM67,upperdir=/var/lib/docker/overlay2/bda1eaf86f4c4a500aac2418bc45956531e81de6fff829b67452174c891f5f15/diff,workdir=/var/lib/docker/overlay2/bda1eaf86f4c4a500aac2418bc45956531e81de6fff829b67452174c891f5f15/work) 整理mount信息,如下：123456789101112131415merged: /var/lib/docker/overlay2/bda1eaf86f4c4a500aac2418bc45956531e81de6fff829b67452174c891f5f15/merged (联合挂载到此目录下)workdir: /var/lib/docker/overlay2/bda1eaf86f4c4a500aac2418bc45956531e81de6fff829b67452174c891f5f15/work upperdir: /var/lib/docker/overlay2/bda1eaf86f4c4a500aac2418bc45956531e81de6fff829b67452174c891f5f15/diff (第六层 rw)lowerdir: /var/lib/docker/overlay2/l/MXQORLOJXLIRFVZ3R26TNWWURM (第5层 init层 ro) /var/lib/docker/overlay2/l/HMOMCX3OAMM4K6KUZORQ7NTIG4 (第四层 ro) /var/lib/docker/overlay2/l/OJ5PMEVISSBQ4X4N2U66YEE6IH (第三层 ro) /var/lib/docker/overlay2/l/JZPHZPG7ZLX7GTKCFVHQZDEF43 (第二层 ro) /var/lib/docker/overlay2/l/2AQ2F7X67IL4TATXIIYO62CM67 (rootfs 第一层 ro) 各个rootfs层文件内容介绍查看rootfs第一层的目录信息1234567[root@www ~]# ll /var/lib/docker/overlay2/l/2AQ2F7X67IL4TATXIIYO62CM67lrwxrwxrwx 1 root root 72 Apr 21 00:32 /var/lib/docker/overlay2/l/2AQ2F7X67IL4TATXIIYO62CM67 -&gt; ../153f0c95b638414d41bb07b9d45243a0219719e468dd9dcb097b80f837b4d8b3/diff[root@www ~]# ll /var/lib/docker/overlay2/153f0c95b638414d41bb07b9d45243a0219719e468dd9dcb097b80f837b4d8b3/total 8drwxr-xr-x 21 root root 4096 Apr 21 00:32 diff-rw-r--r-- 1 root root 26 Apr 21 00:32 link 可以看到，最低层只有两个文件，一个是diff，存放当前层的文件和目录，link则和l目录的软链接向对应123456789101112131415161718192021222324[root@www ~]# ll /var/lib/docker/overlay2/153f0c95b638414d41bb07b9d45243a0219719e468dd9dcb097b80f837b4d8b3/difftotal 76drwxr-xr-x 2 root root 4096 Mar 5 12:46 bindrwxr-xr-x 2 root root 4096 Apr 11 2014 bootdrwxr-xr-x 3 root root 4096 Mar 5 12:45 devdrwxr-xr-x 61 root root 4096 Mar 5 12:46 etcdrwxr-xr-x 2 root root 4096 Apr 11 2014 homedrwxr-xr-x 12 root root 4096 Mar 5 12:46 libdrwxr-xr-x 2 root root 4096 Mar 5 12:45 lib64drwxr-xr-x 2 root root 4096 Mar 5 12:45 mediadrwxr-xr-x 2 root root 4096 Apr 11 2014 mntdrwxr-xr-x 2 root root 4096 Mar 5 12:45 optdrwxr-xr-x 2 root root 4096 Apr 11 2014 procdrwx------ 2 root root 4096 Mar 5 12:46 rootdrwxr-xr-x 7 root root 4096 Mar 5 12:46 rundrwxr-xr-x 2 root root 4096 Mar 5 12:46 sbindrwxr-xr-x 2 root root 4096 Mar 5 12:45 srvdrwxr-xr-x 2 root root 4096 Mar 13 2014 sysdrwxrwxrwt 2 root root 4096 Mar 5 12:46 tmpdrwxr-xr-x 10 root root 4096 Mar 5 12:45 usrdrwxr-xr-x 11 root root 4096 Mar 5 12:46 var[root@www ~]# cat /var/lib/docker/overlay2/153f0c95b638414d41bb07b9d45243a0219719e468dd9dcb097b80f837b4d8b3/link 2AQ2F7X67IL4TATXIIYO62CM67 查看rootfs第二层信息12345678[root@www ~]# ll /var/lib/docker/overlay2/l/JZPHZPG7ZLX7GTKCFVHQZDEF43lrwxrwxrwx 1 root root 72 Apr 21 00:32 /var/lib/docker/overlay2/l/JZPHZPG7ZLX7GTKCFVHQZDEF43 -&gt; ../658b3e84761843f58ecaf82e1f987bf32d498b7bd54a9cf40bf6bf635fff8ac3/diff[root@www ~]# ll /var/lib/docker/overlay2/658b3e84761843f58ecaf82e1f987bf32d498b7bd54a9cf40bf6bf635fff8ac3total 16drwxr-xr-x 6 root root 4096 Apr 21 00:32 diff-rw-r--r-- 1 root root 26 Apr 21 00:32 link-rw-r--r-- 1 root root 28 Apr 21 00:32 lowerdrwx------ 2 root root 4096 Apr 21 00:32 work 第二层有四个文件，diff 和 link如上所序一样。lower文件的内容是当前层下面的rootfs的软连接名1234567891011# 第二层rootfs的lower内容[root@www ~]# cat /var/lib/docker/overlay2/658b3e84761843f58ecaf82e1f987bf32d498b7bd54a9cf40bf6bf635fff8ac3/lower l/2AQ2F7X67IL4TATXIIYO62CM67#第三层rootfs的lower内容[root@www ~]# cat /var/lib/docker/overlay2/0734cd8060d194cf3db93162b421e4f245151920f0b9acda5313ec9f671bc5cc/lower l/JZPHZPG7ZLX7GTKCFVHQZDEF43:l/2AQ2F7X67IL4TATXIIYO62CM67#第四层rootfs的lower内容[root@www ~]# cat /var/lib/docker/overlay2/9ff7c96e688b25a7353c83904d791eae357c2e16315ecbe602009678965ce761/lower l/OJ5PMEVISSBQ4X4N2U66YEE6IH:l/JZPHZPG7ZLX7GTKCFVHQZDEF43:l/2AQ2F7X67IL4TATXIIYO62CM67 最顶层，也就是upperdir层，看一下它的文件目录12[root@www bda1eaf86f4c4a500aac2418bc45956531e81de6fff829b67452174c891f5f15]# lsdiff link lower merged work upperdir是容器层，是可读写的。在容器中所有修改文件操作最后都在upperdir的diff目录体现，并合并到merged目录下。merged目录是联合后挂载的目录，也是容器的文件系统。123456789101112131415# 假如我docker run -it ubuntu:14.04 /bin/bash 启动了一个容器，然后再其/(根目录)下创建一个Test目录,并在/root目录下新建了一个aa文件，并删除了/bin/ss文件。我们现在看一下upperdir的`diff`目录[root@www bda1eaf86f4c4a500aac2418bc45956531e81de6fff829b67452174c891f5f15]# tree diff/diff/├── bin│ └── ss├── root│ └── aa└── Test3 directories, 2 files# 在看一个merged目录，它是叠加后一个完整的文件系统目录结构[root@www bda1eaf86f4c4a500aac2418bc45956531e81de6fff829b67452174c891f5f15]# ls merged/bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys Test tmp usr var OverlayFS constructsOverlayFS将单个Linux主机上的两个目录合并成一个目录。这些目录被称为层，统一过程被称为联合挂载。OverlayFS底层目录称为lowerdir， 高层目录称为upperdir。合并统一视图称为merged。当需要修改一个文件时，使用CoW将文件从只读的Lower复制到可写的Upper进行修改，结果也保存在Upper层。在Docker中，底下的只读层就是image，可写层就是Container 下图分层图，镜像层是lowdir，容器层是upperdir，统一的视图层是merged层 OverlayFs constructs]]></content>
      <tags>
        <tag>docker</tag>
        <tag>overlay2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装Docker CE]]></title>
    <url>%2F2019%2F04%2F20%2Finstall-docker%2F</url>
    <content type="text"><![CDATA[官网安装文档 卸载老版本老版本的Docker，以前叫做docker,docker.io和docker-engine，如果系统里安装了它们，就先卸载掉1$ sudo apt-get remove docker docker-engine docker.io containerd runc 支持的存储系统Docker CE在Ubuntu上支持overlay2，aufs,和btrfs文件系统。在Linux内核4.0或更高的内核版本上，默认使用overlay2文件系统，它的性能能高于aufs。如果非要使用aufs，请见配置Docker CE使用aufs文件系统。 安装Docker CE1.使用apt repository添加仓库123456789101112131415161718192021222324252627# 更新 `apt`$ sudo apt-get update# 安装一些必要的软件$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ gnupg-agent \ software-properties-common# 下载Docker官方GPG key$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -# 检验指纹 `9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88`$ sudo apt-key fingerprint 0EBFCD88 pub rsa4096 2017-02-22 [SCEA] 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88uid [ unknown] Docker Release (CE deb) &lt;docker@docker.com&gt;sub rsa4096 2017-02-22 [S]# 添加仓库$ sudo add-apt-repository \ &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \ $(lsb_release -cs) \ stable&quot; 安装Docker CE1234567891011121314151617$ sudo apt-get update# 默认安装最新版本$ sudo apt-get install docker-ce docker-ce-cli containerd.io# 可以安装指定版本# 1.查看可用版本$ apt-cache madison docker-ce docker-ce | 5:18.09.1~3-0~ubuntu-xenial | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 5:18.09.0~3-0~ubuntu-xenial | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 18.06.1~ce~3-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 18.06.0~ce~3-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages ...# 2.安装指定版本$ sudo apt-get install docker-ce=&lt;VERSION_STRING&gt; docker-ce-cli=&lt;VERSION_STRING&gt; containerd.io 2.使用deb包安装1.去 https://download.docker.com/linux/ubuntu/dists/ 网站，选择合适的.deb文件2.安装.deb包1$ sudo dpkg -i /path/to/package.deb 3.使用shell脚本安装 脚本在 get.docker.com 不建议在生产环境直接使用脚本安装docker 安装步骤1234$ curl -fsSL https://get.docker.com -o get-docker.sh$ sudo sh get-docker.sh&lt;output truncated&gt; 添加非root用户到docker组1$ sudo usermod -aG docker your-user 卸载Docker CE1.卸载Docker CE1$ sudo apt-get purge docker-ce 2.镜像、数据卷、容器和一些自定义的配置文件不会自动删除，需要手动删除1$ sudo rm -rf /var/lib/docker]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker运维相关]]></title>
    <url>%2F2019%2F04%2F20%2Fabout-use-docker%2F</url>
    <content type="text"><![CDATA[记录Docker在日常学习和工作中的相关积累 Docker运维 安装Docker Image相关 Unionfs *overlay2]]></content>
      <tags>
        <tag>docker</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自己动手写docker]]></title>
    <url>%2F2019%2F04%2F20%2Fwrite-docker-self%2F</url>
    <content type="text"><![CDATA[Docker目前是后端服务中最火的技术之一，读这本书《自己动手写docker》，对自己来说，主要是从原理上熟悉docker，顺便复习go语言 容器技术的发展 docker VS 虚拟机 基础技术 docker namespace UTS USER docker cgroup aufs 构建容器 构建实现run命令的容器]]></content>
      <tags>
        <tag>docker</tag>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解容器镜像]]></title>
    <url>%2F2019%2F04%2F18%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[Namespace和Cgroups正如前面所讲，“容器的本质只是一个特殊的进程”。主要是利用Namespace和Cgroup的特性。正如前面所讲，Namespace的作用是“隔离”，它让应用程序只能看到Namespace内的“世界”；而Cgroups的作用是“限制”，它限制“世界”使用某些资源。经过这么一折腾，进程就被“装”在了一个与世隔绝的房间里，而这些房间就是Paas项目赖以生存的应用“沙盒”。 Mount Namespace可是，还有一个问题不知道你有没有仔细思考过：这个房间四周虽然有了墙，但是如果容器进程低头一看地面，又是怎样一副景象呢？ 可能你立即想到了,这一定是一个Mount Namespace的问题：容器里的应用进程，理应当看到了一份完全独立的文件系统，这样它就可以在自己容器目录(/tmp)下进行操作，而完全不受宿主主机以及其他容器的影响。 那么，真实情况是这样吗？ “左耳朵耗子”叔在多年前写的一篇关于 Docker 基础知识的博客里，曾经介绍过一段小程序。这段小程序的作用是，在创建子进程时开启指定的 Namespace。 123456789101112131415161718192021222324252627282930313233#define _GNU_SOURCE#include &lt;sys/mount.h&gt; #include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;stdio.h&gt;#include &lt;sched.h&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt;#define STACK_SIZE (1024 * 1024)static char container_stack[STACK_SIZE];char* const container_args[] = &#123; &quot;/bin/bash&quot;, NULL&#125;;int container_main(void* arg)&#123; printf(&quot;Container - inside the container!\n&quot;); execv(container_args[0], container_args); printf(&quot;Something&apos;s wrong!\n&quot;); return 1;&#125;int main()&#123; printf(&quot;Parent - start a container!\n&quot;); int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL); waitpid(container_pid, NULL, 0); printf(&quot;Parent - container stopped!\n&quot;); return 0;&#125; 这段代码的功能是：在main函数中，通过clone()系统调用创建一个新的子进程 container_main，并且声明要为它启用Mount Namespace（即：CLONE_NEWNS标志） 而这个子进程执行的，是一个“/bin/bash”程序，也就是一个 shell。所以这个 shell 就运行在了 Mount Namespace 的隔离环境中。 编译运行这个程序1234$ make ns$ ./nsParent - start a container!Container - inside the container! 这样，我们就进入了这个“容器”当中。可是，如果在“容器”里执行一下 ls 指令的话，我们就会发现一个有趣的现象： /tmp 目录下的内容跟宿主机的内容是一样的。 12$ ls /tmp# 你会看到好多宿主机的文件 也就是说： 即使开启了 Mount Namespace，容器进程看到的文件系统也跟宿主机完全一样。 这是怎么回事呢？ 其实这并不难理解：Mount Namespace修改的，是容器进程对“挂载点”的认知。但是这就意味着，只有在“挂载点”这个操作发生之后，进程的视图才会被改变。而在此之前，新创建的容器进程挂载点是直接继承宿主主机的各个挂载点。 这时，你可能已经想到了一个解决办法：创建新进程时，除了声明要启用 Mount Namespace 之外，我们还可以告诉容器进程，有哪些目录需要重新挂载，就比如这个 /tmp 目录。于是，我们在容器进程执行前可以添加一步重新挂载 /tmp 目录的操作： 12345678910int container_main(void* arg)&#123; printf(&quot;Container - inside the container!\n&quot;); // 如果你的机器的根目录的挂载类型是 shared，那必须先重新挂载根目录 // mount(&quot;&quot;, &quot;/&quot;, NULL, MS_PRIVATE, &quot;&quot;); mount(&quot;none&quot;, &quot;/tmp&quot;, &quot;tmpfs&quot;, 0, &quot;&quot;); execv(container_args[0], container_args); printf(&quot;Something&apos;s wrong!\n&quot;); return 1;&#125; 可以看到，在修改后的代码里，我在容器进程启动之前，加上了一句 mount(“none”, “/tmp”, “tmpfs”, 0, “”) 语句。就这样，我告诉了容器以 tmpfs（内存盘）格式，重新挂载了 /tmp 目录。 这段修改后的代码，编译执行后的结果又如何呢？我们可以试验一下： 12345$ gcc -o ns ns.c$ ./nsParent - start a container!Container - inside the container!$ ls /tmp 可以看到，这次 /tmp 变成了一个空目录，这意味着重新挂载生效了。我们可以用 mount -l 检查一下： 12$ mount -l | grep tmpfsnone on /tmp type tmpfs (rw,relatime) 可以看到，容器里的 /tmp 目录是以 tmpfs 方式单独挂载的。 更重要的是，因为我们创建的新进程启用了 Mount Namespace，所以这次重新挂载的操作，只在容器进程的 Mount Namespace 中有效。如果在宿主机上用 mount -l 来检查一下这个挂载，你会发现它是不存在的： 12# 在宿主机上$ mount -l | grep tmpfs 这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。 可是，作为一个普通用户，我们希望的是一个更友好的情况：每当创建一个新容器时，我希望容器进程看到的文件系统就是一个独立的隔离环境，而不是继承自宿主机的文件系统。怎么才能做到这一点呢？ 不难想到，我们可以在容器进程启动之前重新挂载它的整个根目录“/”。而由于 Mount Namespace 的存在，这个挂载对宿主机不可见，所以容器进程就可以在里面随便折腾了。 为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 “ls /“ 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。 而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。 所以，一个最常见的 rootfs，或者说容器镜像，会包括如下所示的一些目录和文件，比如 /bin，/etc，/proc 等等： 12ls /bin dev etc home lib lib64 mnt opt proc root run sbin sys tmp usr var 而你进入容器之后执行的 /bin/bash，就是 /bin 目录下的可执行文件，与宿主机的 /bin/bash 完全不同。 现在，你应该可以理解，对 Docker 项目来说，它最核心的原理实际上就是为待创建的用户进程： 启用 Linux Namespace 配置； 设置指定的 Cgroups 参数； 切换进程的根目录（Change Root）。 这样，一个完整的容器就诞生了。不过，Docker 项目在最后一步的切换上会优先使用 pivot_root 系统调用，如果系统不支持，才会使用 chroot。这两个系统调用虽然功能类似，但是也有细微的区别，这一部分小知识就交给你课后去探索了。 rootfs另外，需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。 所以说，rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。那么，对于容器来说，这个操作系统的“灵魂”又在哪里呢？实际上，同一台机器上的所有容器，都共享宿主机操作系统的内核。 这就意味着，如果你的应用程序需要配置内核参数、加载额外的内核模块，以及跟内核进行直接的交互，你就需要注意了：这些操作和依赖的对象，都是宿主机操作系统的内核，它对于该机器上的所有容器来说是一个“全局变量”，牵一发而动全身。 这也是容器相比于虚拟机的主要缺陷之一：毕竟后者不仅有模拟出来的硬件机器充当沙盒，而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾。 不过，正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性：一致性。 什么是容器的“一致性”呢？ 我在专栏的第一篇文章《小鲸鱼大事记（一）：初出茅庐》中曾经提到过：由于云端与本地服务器环境不同，应用的打包过程，一直是使用 PaaS 时最“痛苦”的一个步骤。 但有了容器之后，更准确地说，有了容器镜像（即 rootfs）之后，这个问题被非常优雅地解决了。 由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。 事实上，对于大多数开发者而言，他们对应用依赖的理解，一直局限在编程语言层面。比如 Golang 的 Godeps.json。但实际上，一个一直以来很容易被忽视的事实是，对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库”。 有了容器镜像“打包操作系统”的能力，这个最基础的依赖环境也终于变成了应用沙盒的一部分。这就赋予了容器所谓的一致性：无论在本地、云端，还是在一台任何地方的机器上，用户只需要解压打包好的容器镜像，那么这个应用运行所需要的完整的执行环境就被重现出来了。 这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟。 不过，这时你可能已经发现了另一个非常棘手的问题：难道我每开发一个应用，或者升级一下现有的应用，都要重复制作一次 rootfs 吗？ 比如，我现在用 Ubuntu 操作系统的 ISO 做了一个 rootfs，然后又在里面安装了 Java 环境，用来部署我的 Java 应用。那么，我的另一个同事在发布他的 Java 应用时，显然希望能够直接使用我安装过 Java 环境的 rootfs，而不是重复这个流程。 一种比较直观的解决办法是，我在制作 rootfs 的时候，每做一步“有意义”的操作，就保存一个 rootfs 出来，这样其他同事就可以按需求去用他需要的 rootfs 了。 但是，这个解决办法并不具备推广性。原因在于，一旦你的同事们修改了这个 rootfs，新旧两个 rootfs 之间就没有任何关系了。这样做的结果就是极度的碎片化。 那么，既然这些修改都基于一个旧的 rootfs，我们能不能以增量的方式去做这些修改呢？这样做的好处是，所有人都只需要维护相对于 base rootfs 修改的增量内容，而不是每次修改都制造一个“fork”。 答案当然是肯定的。 这也正是为何，Docker 公司在实现 Docker 镜像时并没有沿用以前制作 rootfs 的标准流程，而是做了一个小小的创新： Docker 在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。 当然，这个想法不是凭空臆造出来的，而是用到了一种叫作联合文件系统（Union File System）的能力。Union File System 也叫 UnionFS，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。比如，我现在有两个目录 A 和 B，它们分别有两个文件： 12345678$ tree.├── A│ ├── a│ └── x└── B ├── b └── x 然后，我使用联合挂载的方式，将这两个目录挂载到一个公共的目录 C 上：12$ mkdir C$ mount -t aufs -o dirs=./A:./B none ./C 这时，我再查看目录 C 的内容，就能看到目录 A 和 B 下的文件被合并到了一起：12345$ tree ./C./C├── a├── b└── x 可以看到，在这个合并后的目录 C 里，有 a、b、x 三个文件，并且 x 文件只有一份。这，就是“合并”的含义。此外，如果你在目录 C 里对 a、b、x 文件做修改，这些修改也会在对应的目录 A、B 中生效。 那么在Docker中如何使用这种Union File System的呢？ 我的环境是 Ubuntu 16.04 和 Docker CE 18.05，这对组合默认使用的是 AuFS 这个联合文件系统的实现。你可以通过 docker info 命令，查看到这个信息。AuFS 的全称是 Another UnionFS，后改名为 Alternative UnionFS，再后来干脆改名叫作 Advance UnionFS。 对于 AuFS 来说，它最关键的目录结构在 /var/lib/docker 路径下的 diff 目录：1/var/lib/docker/aufs/diff/&lt;layer_id&gt; 现在，我们启动一个容器，比如：1$ docker run -d ubuntu:latest sleep 3600 这时候，Docker 就会从 Docker Hub 上拉取一个 Ubuntu 镜像到本地。 这个所谓的“镜像”，实际上就是一个 Ubuntu 操作系统的 rootfs，它的内容是 Ubuntu 操作系统的所有文件和目录。不过，与之前我们讲述的 rootfs 稍微不同的是，Docker 镜像使用的 rootfs，往往由多个“层”组成： 123456789101112$ docker image inspect ubuntu:latest... &quot;RootFS&quot;: &#123; &quot;Type&quot;: &quot;layers&quot;, &quot;Layers&quot;: [ &quot;sha256:f49017d4d5ce9c0f544c...&quot;, &quot;sha256:8f2b771487e9d6354080...&quot;, &quot;sha256:ccd4d61916aaa2159429...&quot;, &quot;sha256:c01d74f99de40e097c73...&quot;, &quot;sha256:268a067217b5fe78e000...&quot; ] &#125; 可以看到，这个 Ubuntu 镜像，实际上由五个层组成。这五个层就是五个增量 rootfs，每一层都是 Ubuntu 操作系统文件与目录的一部分；而在使用镜像时，Docker 会把这些增量联合挂载在一个统一的挂载点上（等价于前面例子里的“/C”目录）。这个挂载点就是 /var/lib/docker/aufs/mnt/，比如： 1/var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e 不出意外的，这个目录里面正是一个完整的 Ubuntu 操作系统： 12$ ls /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3ebin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 那么，前面提到的五个镜像层，又是如何被联合挂载成这样一个完整的 Ubuntu 文件系统的呢？这个信息记录在 AuFS 的系统目录 /sys/fs/aufs 下面。首先，通过查看 AuFS 的挂载信息，我们可以找到这个目录对应的 AuFS 的内部 ID（也叫：si）： 12$ cat /proc/mounts| grep aufsnone /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fc... aufs rw,relatime,si=972c6d361e6b32ba,dio,dirperm1 0 0 即，si=972c6d361e6b32ba。然后使用这个 ID，你就可以在 /sys/fs/aufs 下查看被联合挂载在一起的各个层的信息： 12345678$ cat /sys/fs/aufs/si_972c6d361e6b32ba/br[0-9]*/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...=rw/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...-init=ro+wh/var/lib/docker/aufs/diff/32e8e20064858c0f2...=ro+wh/var/lib/docker/aufs/diff/2b8858809bce62e62...=ro+wh/var/lib/docker/aufs/diff/20707dce8efc0d267...=ro+wh/var/lib/docker/aufs/diff/72b0744e06247c7d0...=ro+wh/var/lib/docker/aufs/diff/a524a729adadedb90...=ro+wh 从这些信息里，我们可以看到，镜像的层都放置在 /var/lib/docker/aufs/diff 目录下，然后被联合挂载在 /var/lib/docker/aufs/mnt 里面。而且，从这个结构可以看出来，这个容器的 rootfs 由如下图所示的三部分组成： 这里有一个图 第一部分，只读层。它是这个容器的 rootfs 最下面的五层，对应的正是 ubuntu:latest 镜像的五层。可以看到，它们的挂载方式都是只读的（ro+wh，即 readonly+whiteout，至于什么是 whiteout，我下面马上会讲到）。这时，我们可以分别查看一下这些层的内容： 123456$ ls /var/lib/docker/aufs/diff/72b0744e06247c7d0...etc sbin usr var$ ls /var/lib/docker/aufs/diff/32e8e20064858c0f2...run$ ls /var/lib/docker/aufs/diff/a524a729adadedb900...bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 可以看到，这些层，都以增量的方式分别包含了 Ubuntu 操作系统的一部分。 第二部分，可读写层。 它是这个容器的 rootfs 最上面的一层（6e3be5d2ecccae7cc），它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。可是，你有没有想到这样一个问题：如果我现在要做的，是删除只读层里的一个文件呢？为了实现这样的删除操作，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。比如，你要删除只读层里一个名叫 foo 的文件，那么这个删除操作实际上是在可读写层创建了一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文件“遮挡”起来，“消失”了。这个功能，就是“ro+wh”的挂载方式，即只读 +whiteout 的含义。我喜欢把 whiteout 形象地翻译为：“白障”。所以，最上面这个可读写层的作用，就是专门用来存放你修改 rootfs 后产生的增量，无论是增、删、改，都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用 docker commit 和 push 指令，保存这个被修改过的可读写层，并上传到 Docker Hub 上，供其他人使用；而与此同时，原先的只读层里的内容则不会有任何变化。这，就是增量 rootfs 的好处。 第三部分，Init 层。 它是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。需要这样一层的原因是，这些文件本来属于只读的 Ubuntu 镜像的一部分，但是用户往往需要在启动容器时写入一些指定的值比如 hostname，所以就需要在可读写层对它们进行修改。可是，这些修改往往只对当前的容器有效，我们并不希望执行 docker commit 时，把这些信息连同可读写层一起提交掉。所以，Docker 做法是，在修改了这些文件之后，以一个单独的层挂载了出来。而用户执行 docker commit 只会提交可读写层，所以是不包含这些内容的。最终，这 7 个层都被联合挂载到 /var/lib/docker/aufs/mnt 目录下，表现为一个完整的 Ubuntu 操作系统供容器使用。 总结在今天的分享中，我着重介绍了 Linux 容器文件系统的实现方式。而这种机制，正是我们经常提到的容器镜像，也叫作：rootfs。它只是一个操作系统的所有文件和目录，并不包含内核，最多也就几百兆。而相比之下，传统虚拟机的镜像大多是一个磁盘的“快照”，磁盘有多大，镜像就至少有多大。 通过结合使用 Mount Namespace 和 rootfs，容器就能够为进程构建出一个完善的文件系统隔离环境。当然，这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力。 而在 rootfs 的基础上，Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案，这就是容器镜像中“层”的概念。 通过“分层镜像”的设计，以 Docker 镜像为核心，来自不同公司、不同团队的技术人员被紧密地联系在了一起。而且，由于容器镜像的操作是增量式的，这样每次镜像拉取、推送的内容，比原本多个完整的操作系统的大小要小得多；而共享层的存在，可以使得所有这些容器镜像需要的总空间，也比每个镜像的总和要小。这样就使得基于容器镜像的团队协作，要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多。更重要的是，一旦这个镜像被发布，那么你在全世界的任何一个地方下载这个镜像，得到的内容都完全一致，可以完全复现这个镜像制作者当初的完整环境。这，就是容器技术“强一致性”的重要体现。 而这种价值正是支撑 Docker 公司在 2014~2016 年间迅猛发展的核心动力。容器镜像的发明，不仅打通了“开发 - 测试 - 部署”流程的每一个环节，更重要的是： 容器镜像将会成为未来软件的主流发布方式。]]></content>
      <tags>
        <tag>docker</tag>
        <tag>image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C程序分析工具 valgrind]]></title>
    <url>%2F2019%2F04%2F18%2F04-introducing-valgrind%2F</url>
    <content type="text"><![CDATA[现在介绍另外一个工具，在学习C的过程中，要习惯性的使用，它就是 Valgrind 。Valgrind是一个运行你的程序的程序，并且随后会报告所有你犯下的可怕错误。 安装 Valgrind有两种安装方式，在Centos中，可以通过yum install -y Valgrind方式安装，另外是下载Valgrind源码安装。 这里演示源码安装方式 12345678910111213141) 下载Valgrind源码包wget wget https://sourceware.org/pub/valgrind/valgrind-3.15.0.tar.bz22) 解压tar jxvf valgrind-3.15.0.tar.bz23) configure./configure4) 编译make5) 安装make install 使用Valgrind这里写一个带有错误的C程序，待会儿让Valgrind来运行一下,error.c源码如下： 1234567891011#include&lt;stdio.h&gt;int main()&#123; int age = 12; int height; printf(&quot;I am %d years old.\n&quot;); printf(&quot;I am %d inches tall.\n&quot;,height); return 0;&#125; 当前程序有两个错误： 没有初始化height变量 没有将age变量传入第一个printf函数 使用make构建 123456789101112[root@www practice04]# makecc -Wall -g error.c -o errorerror.c: In function ‘main’:error.c:7:5: warning: format ‘%d’ expects a matching ‘int’ argument [-Wformat=] printf(&quot;I am %d years old.\n&quot;); ^error.c:4:9: warning: unused variable ‘age’ [-Wunused-variable] int age = 12; ^error.c:8:11: warning: ‘height’ is used uninitialized in this function [-Wuninitialized] printf(&quot;I am %d inches tall.\n&quot;,height); ^ 使用Valgrind来运行error程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849==17432== Memcheck, a memory error detector==17432== Copyright (C) 2002-2017, and GNU GPL&apos;d, by Julian Seward et al.==17432== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info==17432== Command: ./error==17432== I am -16775928 years old.==17432== Conditional jump or move depends on uninitialised value(s)==17432== at 0x4E7C9F2: vfprintf (in /usr/lib64/libc-2.17.so)==17432== by 0x4E86878: printf (in /usr/lib64/libc-2.17.so)==17432== by 0x400561: main (error.c:8)==17432== ==17432== Use of uninitialised value of size 8==17432== at 0x4E7BE8B: _itoa_word (in /usr/lib64/libc-2.17.so)==17432== by 0x4E7CF05: vfprintf (in /usr/lib64/libc-2.17.so)==17432== by 0x4E86878: printf (in /usr/lib64/libc-2.17.so)==17432== by 0x400561: main (error.c:8)==17432== ==17432== Conditional jump or move depends on uninitialised value(s)==17432== at 0x4E7BE95: _itoa_word (in /usr/lib64/libc-2.17.so)==17432== by 0x4E7CF05: vfprintf (in /usr/lib64/libc-2.17.so)==17432== by 0x4E86878: printf (in /usr/lib64/libc-2.17.so)==17432== by 0x400561: main (error.c:8)==17432== ==17432== Conditional jump or move depends on uninitialised value(s)==17432== at 0x4E7CF54: vfprintf (in /usr/lib64/libc-2.17.so)==17432== by 0x4E86878: printf (in /usr/lib64/libc-2.17.so)==17432== by 0x400561: main (error.c:8)==17432== ==17432== Conditional jump or move depends on uninitialised value(s)==17432== at 0x4E7CABD: vfprintf (in /usr/lib64/libc-2.17.so)==17432== by 0x4E86878: printf (in /usr/lib64/libc-2.17.so)==17432== by 0x400561: main (error.c:8)==17432== ==17432== Conditional jump or move depends on uninitialised value(s)==17432== at 0x4E7CB40: vfprintf (in /usr/lib64/libc-2.17.so)==17432== by 0x4E86878: printf (in /usr/lib64/libc-2.17.so)==17432== by 0x400561: main (error.c:8)==17432== I am 0 inches tall.==17432== ==17432== HEAP SUMMARY:==17432== in use at exit: 0 bytes in 0 blocks==17432== total heap usage: 0 allocs, 0 frees, 0 bytes allocated==17432== ==17432== All heap blocks were freed -- no leaks are possible==17432== ==17432== Use --track-origins=yes to see where uninitialised values come from==17432== For lists of detected and suppressed errors, rerun with: -s==17432== ERROR SUMMARY: 6 errors from 6 contexts (suppressed: 0 from 0) Valgrind运行并分析程序，返回的数据分成三部分，第一部分是Vargrind版本信息,第二部分报告程序的相关错误信息，第三部分会生成一个简短报告，告诉你你的程序有多烂。 附加题 根据Valgrind错误信息，修复程序 12345678910111213141516==17515== Memcheck, a memory error detector==17515== Copyright (C) 2002-2017, and GNU GPL&apos;d, by Julian Seward et al.==17515== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info==17515== Command: ./error==17515== I am 12345 years old.I am 72 inches tall.==17515== ==17515== HEAP SUMMARY:==17515== in use at exit: 0 bytes in 0 blocks==17515== total heap usage: 0 allocs, 0 frees, 0 bytes allocated==17515== ==17515== All heap blocks were freed -- no leaks are possible==17515== ==17515== For lists of detected and suppressed errors, rerun with: -s==17515== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)]]></content>
      <tags>
        <tag>C</tag>
        <tag>valgrind</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[格式化输出]]></title>
    <url>%2F2019%2F04%2F18%2F03-format-print%2F</url>
    <content type="text"><![CDATA[printf格式化函数许多编程语言都使用了C风格格式化输出，所以让我们也尝试一下,编写format-output.c，代码如下： 1234567891011#include&lt;stdio.h&gt;int main()&#123; int age = 10; int height = 72; printf(&quot;I am %d years old.\n&quot;,age); printf(&quot;I am %d inches tall.\n&quot;,height); return 0;&#125; Makefile文件如下：123456CFLAGS=-Wall -gall: format-outputclean: rm -rf format-output 执行make，将看到如下内容:12[root@www practice03]# makecc -Wall -g format-output.c -o format-output 这段代码量很小，我们逐行分析一下： 首先你包含了一个叫做 stdio.h 的头文件。这告诉你的编译器要使用“标准的输入/输出函数”。他们之一就是下面的printf。 然后你声明了一个int类型的age变量，并赋值为10。 接着你又声明了一个int类型的height变量，并赋值为72。 再然后用printf函数来打印你的年龄和身高 在printf中你会注意到你传入了一个字符串，这就是格式字符串，和其它语言中一样。 在格式化字符串后面，你传入了一些变量，他们应该被printf“替换”进格式化字符串中。 附加题 执行 man 3 printf 来阅读它更多可用的“%”格式的占位符。 控制符 说明 %d 按十进制整型数据的实际长度输出。 %ld 输出长整型数据。 %md m为指定的输出字段的宽度。如果数据的位数小于 m，则左端补以空格，若大于 m，则按实际位数输出。 %u 输出无符号整型（unsigned）。输出无符号整型时也可以用 %d，这时是将无符号转换成有符号数，然后输出。但编程的时候最好不要这么写，因为这样要进行一次转换，使 CPU 多做一次无用功。 %c 输出一个字符 %f 用来输出实数，包括单精度和双精度，以小数形式输出。不指定字段宽度，由系统自动指定，整数部分全部输出，小数部分输出 6 位，超过 6 位的四舍五入。 %.mf 输出实数时小数点后保留 m 位，注意 m 前面有个点。 %o 以八进制整数形式输出，这个就用得很少了，了解一下就行了。 %s 用来输出字符串。用 %s 输出字符串同前面直接输出字符串是一样的。但是此时要先定义字符数组或字符指针存储或指向字符串。 %x（或 %X 或 %#x 或 %#X） 以十六进制形式输出整数，这个很重要。 %x、%X、%#x、%#X 的区别 一定要掌握 %x（或 %X 或 %#x 或 %#X），因为调试的时候经常要将内存中的二进制代码全部输出，然后用十六进制显示出来。下面写一个程序看看它们四个有什么区别： 12345678910# include &lt;stdio.h&gt;int main(void)&#123; int i = 47; printf(&quot;%x\n&quot;, i); printf(&quot;%X\n&quot;, i); printf(&quot;%#x\n&quot;, i); printf(&quot;%#X\n&quot;, i); return 0;&#125; 在 VC++ 6.0 中的输出结果：12342f2F0x2f0X2F 从输出结果可以看出：如果是小写的x，输出的字母就是小写的；如果是大写的X，输出的字母就是大写的；如果加一个#，就以标准的十六进制形式输出。 最好是加一个#，否则如果输出的十六进制数正好没有字母的话会误认为是一个十进制数呢！]]></content>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker:从进程说起]]></title>
    <url>%2F2019%2F04%2F12%2Fdocker-process%2F</url>
    <content type="text"><![CDATA[进程进程是运行的程序，一旦“程序”被执行起来，它就从磁盘上的二进制文件，变成了计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。像这样一个程序运起来后的计算机执行环境的总和，就是进程。 Namespace而容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。 对于 Docker 等大多数 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而Namespace 技术则是用来修改进程视图的主要方法。 假设你已经有了一个 Linux 操作系统上的 Docker 项目在运行，比如我的环境是 Ubuntu 16.04 和 Docker CE 18.05。 接下来，让我们创建一个容器试试。 12$ docker run -it busybox /bin/bash/ # 这就是大名鼎鼎的docker run。-it参数告诉Docker项目在启动的时候分配一个输入/输出环境，也就是TTY，跟容器的标准输入相关联，这样我们就可以和这个容器交互了。而/bin/bash就是在这个容器里运行的程序。 所以，上面这条指令翻译成人类的语言就是：请帮我启动一个容器，在容器里执行 /bin/sh，并且给我分配一个命令行终端跟这个容器交互。 在容器里执行ps，会出现如下信息： 1234/ # psPID USER TIME COMMAND 1 root 0:00 /bin/sh 10 root 0:00 ps 我们可以看到，容器启动时执行的/bin/bash，就是这个容器里第1号进程（PID=1）,当前，容器里有两个进程在运行。这两个进程，已经被Docker隔离在了一个跟宿主主机完全不同的世界当中。 那么，这究竟是这么做到的呢？ 本来，当我们在宿主主机执行/bin/bash,操作系统会给它分配一个进程编号，比如PID=100。而现在，我们通过Docker把/bin/bash运行在容器当中。这时候，Docker就会在这个进程启动时，给它实时一个“障眼法”，让它永远也看不到前面的99个进程，这里，它就会误认为自己是第1号进程了。 这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如 PID=1。可实际上，他们在宿主主机的操作系统中，还是原来的第100号进程。 这种技术，就是Linux中的Namespace机制。而Namespace的使用方式也非常有意思：它其实只是Linux创建新进程的一个可选参数。我们知道，在Linux系统中创建线程的系统调用是clone(),比如： 1int pid = clone(main_function, stack_size, SIGCHLD, NULL); 这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。而当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如： 1int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。 而除了刚刚这种PID Namespace，Linux操作系统还提供了Mount、UTS、IPC、Network和User 这些Namespace，用来对各种不同的进程上下文进行“障眼法”操作。 这就是Linux容器最基本的实现原理了。所以说，容器只是一种特殊的进程而已，在创建该容器进程时，指定了这个进程所需要启用的一组Namespace参数。这样，容器就只能“看”到当前Namespace所限定的资源、文件、设备、状态或者配置。而宿主主机以及其他不想关的程序，它就完全看不到了。 CGroup除了Namespace对容器环境进行隔离，还通过Linux CGroup限制容器进程使用相关资源 Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。 在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。在 Ubuntu 16.04 机器里，我可以用 mount 指令把它们展示出来，这条命令是： 1234567cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)...... 可以看到，在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是： 123$ ls /sys/fs/cgroup/cpucgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_releasecgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 如果熟悉 Linux CPU 管理的话，你就会在它的输出里注意到 cfs_period 和 cfs_quota 这样的关键词。这两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。 而这样的配置文件又如何使用呢？你需要在对应的子系统下面创建一个目录，比如，我们现在进入 /sys/fs/cgroup/cpu 目录下： 1234root@ubuntu:/sys/fs/cgroup/cpu$ mkdir containerroot@ubuntu:/sys/fs/cgroup/cpu$ ls container/cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_releasecgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。 现在，我们在后台执行这样一条脚本： 12$ while : ; do : ; done &amp;[1] 226 显然，它执行了一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号（PID）是 226。这样，我们可以用 top 指令来确认一下 CPU 有没有被打满： 12$ top%Cpu0 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 在输出里可以看到，CPU 的使用率已经 100% 了（%Cpu0 :100.0 us）。而此时，我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us）： 1234$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us -1$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us 100000 接下来，我们可以通过修改这些文件的内容来设置限制。比如，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us）： 1$ echo 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 结合前面的介绍，你应该能明白这个操作的含义，它意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该进程生效了： 1$ echo 226 &gt; /sys/fs/cgroup/cpu/container/tasks 我们可以用 top 指令查看一下： 12$ top%Cpu0 : 20.3 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st 可以看到，计算机的 CPU 使用率立刻降到了 20%（%Cpu0 : 20.3 us）。 除 CPU 子系统外，Cgroups 的每一项子系统都有其独有的资源限制能力，比如： blkio，为​​​块​​​设​​​备​​​设​​​定​​​I/O 限​​​制,一般用于磁盘等设备； cpuset，为进程分配单独的 CPU 核和对应的内存节点； memory，为进程设定内存使用的限制。 Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令： 1$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash 在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认： 1234$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 100000$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 20000 这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。 总结一个正在运行的 Docker 容器，其实就是一个启用了多个 Linux Namespace 的应用进程，而这个进程能够使用的资源量，则受 Cgroups 配置的限制。]]></content>
      <tags>
        <tag>Docker</tag>
        <tag>容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用make]]></title>
    <url>%2F2019%2F04%2F09%2F02-use-make%2F</url>
    <content type="text"><![CDATA[使用make使用make的第一个阶段，就是用它已知的方法来构建程序。make预置了一些知识，来从其它文件构建多种文件。在上一个练习中，有如下操作： 12$ make hello-world$ CFLAG=&quot;-Wall&quot; make hello-world 第一条命令中你告诉make，“我想创建名为hello-word的文件”。于是，make执行了下面的动作： 文件hello-world存在吗？ 没有的话。好的，那有没有其它文件是以hello-world开头的？ 有，叫做hello-world.c。我知道如何构建.c文件吗？ 知道。我会运行 cc hello-world.c -o hello-world 来构建它。 我将使用 cc 从 hello-world.c 文件为你构建 hello-world 上面的第二条命令，是向 make 命令传递“修改器”的途径。这个例子中，我执行了 CFLAGS=&quot;-Wall&quot; make hello-world，它会给make使用的 cc 命令添加 -Wall 选项。这行命令告诉编译器要报告所有的警告。 Makefile编写创建文件并写入一下内容 1234CFLAGS=-Wall -gclean: rm -rf hello-world 将文件在你当前文件夹下保存为Makefile。Make会自动假设当前文件夹下有一个叫Makefile的文件，并且会执行它。 首先我们在文件中设置CFLAGS,所以之后都不用再设置了。并且添加了-g标识来获取调试信息。接着我们写了一个clean的部门，它告诉make如何清理我们的小项目。 请确保，你的makefile文件和hello-world.c在同一个目录下，hello-world.c内容如下： 1234int main(int argc,char *argv[])&#123; puts(&quot;Hello world.\nnice to see you!&quot;); return 0;&#125; 之后可以执行命令: 12$ make clean$ make hello-world 如果代码正常执行，你应该看到下面这些内容 12345678[root@www practice02]# make cleanrm -f hello-world[root@www practice02]# make hello-worldcc -Wall -g hello-world.c -o hello-worldhello-world.c: In function ‘main’:hello-world.c:2:5: warning: implicit declaration of function ‘puts’ [-Wimplicit-function-declaration] puts(&quot;Hello world.\nnice to see you!&quot;); ^ 你可以看出来,我执行了make clean，它告诉make执行我们的clean目标。在看makefile，发现clean下面有一些shell命令。你可以在此处输入任意多的命令，所以它是一个非常棒的自动化工具。 注:如果你修改了 ex1.c ，添加了 #include\&lt;stdio> ，输出中的关于 puts 的警告就会消失（这其实应该算作一个错误） 。我这里有警告是因为我并没有去掉它 附加题 创建目标 all:hello-world,可以用单个命令make构建hello-world 123456CFLAGS=-Wall -gall: hello-worldclean: rm -f hello-world 阅读 man make 来了解关于他的更多信息。 阅读 man cc 来了解关于 -Wall 和 -g 行为的更多信息。]]></content>
      <tags>
        <tag>C</tag>
        <tag>Makefile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[启用编译器]]></title>
    <url>%2F2019%2F04%2F09%2F01-dust-off-that-compiler%2F</url>
    <content type="text"><![CDATA[这是一个最简单的C程序,hello-world.c： 1234int main(int argc, char *argv[])&#123; puts("Hello world."); return 0;&#125; 在Linux终端输入： 12$ make hello-worldcc hello-world.c -o hello-world 现在，你可以运行并可以看到程序输出。 12$ ./hello-worldHello World. 思考： 1.make指令的运行原理和流程 2.puts函数的作用 C 库函数 int puts(const char *str) 把一个字符串写入到标准输出 stdout，直到空字符，但不包括空字符。换行符会被追加到输出中。 3.C语言中的空字符 在C语言中空字符用’\0’表示; ‘\0’对应的整数值是0，所以给一个字符变量赋值为空字符时，以下两种都是可以的： 12char ch=&apos;\0&apos;;char ch=0; 4.字符串 字符串就是一串零个或多个字符，并且以位模式为全0的NUL字节即空字符(‘\0’)结尾。C语言中字符串没有显示的数据类型，字符串通常存储在字符数组或动态分配的内存中，在编码操作中通常将整个字符串作为操作对象，常用操作包括复制、查找、比较等。 5.空字符与字符串 1、空字符是字符串的终止符。注：空字符本身不是字符串的一部分，所以字符串的长度并不包含空字符； 2、操作字符串时，必须保证字符串以空字符结尾(注：不以空字符结尾的字符序列，不是字符串)。]]></content>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笨方法学C]]></title>
    <url>%2F2019%2F04%2F09%2Flearn-c-the-hard-way%2F</url>
    <content type="text"><![CDATA[这几年，我一直在学习编程语言，从JAVA、PHP、JS到Go，其实我一直想学好的是C语言，C即是基础。这个《笨方法学C》的读书系列，希望自己一定要坚持下来，好好学习，好好总结。 这本书的目的是让你足够熟悉C语言，并能够使用它编写自己的软件，或者修改其他人的代码。你需要学习下面这些东西来达到这一阶段： C的基本语法和编写习惯; 编译,make文件和链接; 寻找和预防bug; 防御性编程实战; 使C的代码崩溃; 编写基本的Unix系统软件。]]></content>
      <tags>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis源码从哪里读起?]]></title>
    <url>%2F2019%2F04%2F03%2FRedis%E6%BA%90%E7%A0%81%E4%BB%8E%E5%93%AA%E9%87%8C%E8%AF%BB%E8%B5%B7%2F</url>
    <content type="text"><![CDATA[转载至 http://zhangtielei.com/posts/blog-redis-how-to-start.html 溯源Redis使用C语言写的。首先，你应该从main函数读起。但是我们在读的时候应该抓住一条主线，也就是当我们向Redis输入一条命令的时候，代码是如何一步一步执行的。这样我们就可以先从外部观察，尝试执行一些命令，在了解了这些命令执行的外部表现之后，再钻进去看对应的源码是如何实现的。要想读懂这些代码，首先我们需要理解Redis的事件机制。而且，一旦理解了Redis的事件循环（Event Loop）的机制，我们还会搞明白一个有趣的问题：为什么Redis是单线程执行却能同时处理多个请求？（当然严格来说Redis运行并不是只有一个线程，但除了主线程外，Redis的其它线程只是起辅助作用，它们是一些在后台运行做异步耗时任务的线程）。 从main函数开始，沿着代码执行路径，实际上我们可以一直追下去。但为了让本文不至于太过冗长，我们还是限定一下返回。本文的目标就定为：引领读者从main函数开始，一步步追踪下去，最终到达任一Redis命令执行入口。这样接下来就可以与Redis内部数据结构详解的一些列文章衔接上。 为了表述清楚，本文按照如下思路进行： 1.先概括地介绍整个代码初始化流程（从main函数开始）和事件循环的结构； 2.再概括地介绍对于Redis命令请求的处理流程； 3.重点介绍事件机制； 4.对于前面介绍的各个代码流程处理，给出详细的代码调用关系，方便随时查阅； 根据这样几部分的划分，如果你只想粗读大致的处理流程，那么只需要阅读前两个部分就可以了。而后两部分则会深入到某些值得关注的细节。 注：本文的分析基于Redis源码的5.0分支。 初始化流程和事件循环概述Redis源码的main函数在源文件server.c中。main函数开始执行后的逻辑可以分为两个阶段： 各种初始化（包括事件循环的初始化） 执行事件循环。 这两个执行阶段可以用下面的流程图来表达： 图1：main函数执行逻辑 首先，我们开一下初始化阶段中的各个步骤： 配置加载和初始化，这一步表示Redis服务器基本数据结构和各种参数的初始化。在Redis源码中，Redis服务器就是用一个叫做redisServer的struct来表示的。 里面定义了Redis服务器赖以运行的各种参数，比如监听的端口号和文件描述符、当前连接的各个client端、Redis命令表（command table）配置、持久化相关的各种参数，等等。Redis服务器在运行时就是由一个redisServer类型的全局变量来表示的（变量名叫 server）,这一步的初始化主要对于这个全局变量进行初始化。在整个初始化过程中，有一个特别需要注意的函数：populateCommandTable。它初始化了Redis命令表，通过它可以由任意一个Redis命令的名字查找该命令的配置信息（比如该命令接受的命令参数个数、执行函数入口等）。在本文的第二部分，我们将会一起来看看如何从接受一个Redis命令的请求开始，一步步执行到查阅这个命令表，从而找到该命令的执行入口。另外，这一步中还有一个值得一提的地方，在对全局的redisServer结构进行初始化之后，还需要从配置文件（redis.conf）中加载配置。这个过程可能覆盖掉之前的初始化过的redisServer结构中的某些参数。换句话说，就是先经过一轮初始化，保证Redis的各个内部数据结构以及参数都有缺省值，然后再从配置文件中加载自定义的配置。 创建事件循环，在Redis中，事件循环是用一个叫aeEventLoop的struct来表示的。创建事件循环这一步主要就是创建一个aeEventLoop结构，并存储到server全局变量中。另外，事件循环的执行依赖系统低层的I/O多路复用机制，比如Linux系统上的epoll机制。因此，这一步也包含对于低层I/O多路复用机制的初始化。（调用系统API） 开始监听socket。服务器程序需要监听才能收到请求。根据配置，这一步可能会打开两种监听：对于TCP连接的监听和对于Unix domain socket的监听，Unix domain socket是一种高效的进程间通信机制，在POSIX规范中也有明确的定义，用于在同一台主机上的两个不同进程之间进行通信，比使用TCP协议性能跟高（因为省去了协议栈的开销）。当使用Redis客户端连接同一台机器上的Redis服务器时，可以使用Unix domain socket进行连接。但是不管是哪一种监听，程序都会获得文件描述符，并存储到server全局变量中。对于TCP的监听来说，由于监听的IP地址和端口可以绑定多个，因此获得的用于监听TCP连接的文件描述符也可以包含多个。后面，程序就可以拿这一步获得的文件描述符去注册I/O事件回调了。 注册timer事件回调。Redis作为一个单线程(single-threaded)的程序，它如果想调度一些异步执行的任务，比如比如周期性的执行过期key的回收动作，除了依赖事件循环机制，没有其它办法。这一步就是向前面刚刚创建好的事件循环中注册一个timer事件，并配置成可以周期性地执行一个回调函数：serverCron。由于Redis只有一个主线程，因此这个函数周期性的执行也是在这个线程内，它由事件循环来驱动（即在合适的时机调用），但不影响同一个线程上其它逻辑的执行（相当于按时间分片了）。serverCron函数到底做了什么呢？实际上，它除了周期性地执行过期key的回收动作，还执行了很多其它任务，比如主从重连、Cluster节点间的重连、bgsave和aof rewrite的触发执行，等等。 注册I/O事件回调。Redis服务器最主要的工作就是监听I/O事件，从中分析出来自客户端的命令请求，执行命令，然后返回响应结果。对于I/O事件的监听，自然也是依赖事件循环。前面提到过，Redis可以打开两种监听：对于TCP连接的监听和对于Unix domain socket的监听。因此，这里就包含对于这两种I/O事件的回调的注册，两个回调函数分别是acceptTcpHandler和acceptUnixHandler。对于来自Redis客户端的请求的处理，就会走到这两个函数中去。我们在下一部分就会讨论到这个处理过程。另外，其实Redis在这里还会注册一个I/O事件，用于通过管道(pipe[6])机制与module进行双向通信。这个也不是本文的重点，我们暂时忽略它。 初始化后台线程。Redis会创建一些额外的线程，在后台运行，专门用于处理一些耗时的并且可以被延迟执行的任务（一般是一些清理工作）。在Redis里面这些后台线程被称为bio（background i/o server）。它们负责的任务包括：可以延迟执行的文件关闭操作（比如unlink命令的执行），AOF的持久化写库操作（即fsync调用，但注意只有可以被延迟执行的fsync操作才在后台线程执行），还有一些大key的清除操作（比如flushdb async命令的执行）。可见bio这个名字有点名不副实，它做的事情不一定跟I/O有关。对于这些后台线程，我们可能还会产生一个疑问：前面的初始化过程，已经注册了一个timer事件回调，即serverCron函数，按说后台线程执行的这些任务似乎也可以放在serverCron中去执行。因为serverCron函数也是可以用来执行后台任务的。实际上这样做是不行的。前面我们已经提到过，serverCron由事件循环来驱动，执行还是在Redis主线程上，相当于和主线程上执行的其它操作（主要是对于命令请求的执行）按时间进行分片了。这样的话，serverCron里面就不能执行过于耗时的操作，否则它就会影响Redis执行命令的响应时间。因此，对于耗时的、并且可以被延迟执行的任务，就只能放到单独的线程中去执行了。 启动事件循环。前面创建好了事件循环的结构，但还没有真正进入循环的逻辑。过了这一步，事件循环就运行起来，驱动前面注册的timer事件回调和I/O事件回调不断执行。 注意：Redis服务器初始化其实还有很多其它事情，比如加载数据到内存，Cluster集群的初始化，module的初始化，等等。但为了简化，上面讨论的初始化流程，只列出了我们当前关注的步骤。本文关注的是由事件驱动的整个运行机制以及跟命令执行直接相关的部分，因此我们暂时忽略掉其它不太相关的步骤。 现在，我们继续去讨论上面流程图中的第二个阶段：事件循环。 我们先想一下为什么这里需要一个循环。 一个程序启动后，如果没有循环，那么它从第一条指令执行到最后一条指令，然后就只能退出了。而Redis作为一个服务端程序，是要等客户端不停地发来请求然后做相应的处理，不能自己执行完就退出了。因此，Redis启动后必定要进入一个无限循环，显然，程序在每一次循环执行中，如果有事件（包括客户端请求的I/O事件）发生，就会去处理这些事件。如果没有事件发生呢？程序显然也不应该空转，而是应该等待，把整个循环阻塞住。这里的等待，就是上面流程图的【等待事件发生】这个步骤。那么，当整个循环被阻塞住之后，什么时候再恢复执行呢？自然是等待的事件发生的时候，程序被重新唤醒，循环继续下去。这里需要的等待和唤醒操作，是怎么实现呢？它们都需要依赖系统的能力才能做到。 实际上，这种事件循环机制，对于开发过手机客户端的同学来说，是非常常见且基础的机制。比如跑在iOS/Android上面的App，这些程序都有一个消息循环，负责等待各种UI事件（点击、滑动等）的发生，然后进行处理。同理，对应到服务端，这个循环的原理可以认为差不多，只是等待和处理的事件变成是I/O事件了。另外，除了I/O事件，整个系统在运行过程中肯定还需要根据时间来调度执行一些任务，比如延迟100毫秒再执行某个操作，或者周期性地每隔1秒执行某个任务，这就需要等待和处理另外一种事件——timer事件。 timer事件和I/O事件是两种截然不同的事件，如何由事件循环来统一调度呢？假设事件循环有空闲的时候去等待I/O事件的发生，那么有可能一个timer事件先发生了，这时事件循环就没有被及时唤醒（仍在等待I/O事件）；反之，如果事件循环在等待timer事件，而一个I/O事件先发生了，那么同样没能够及时唤醒。因此，我们必须有一种机制能够同时等待这两种事件的发生。而恰好，一些系统的API可以做到这一点（比如我们前面提到的epoll机制）。 前面流程图的第二阶段已经比较清楚地表达出了事件循环的执行流程。在这里我们对于其中一些步骤需要关注的地方做一些补充说明： 查找最近的timer事件。如前所序，事件循环需要等待timerI/O两种事件。对于I/O事件，只需要明确等待的是哪些文件描述符就可以了；而对于timer事件，还需要经过一番比较，明确在当前这一轮循环中需要等待多长时间。由于系统运行过程中可能注册多个timer事件回调，比如先要求在100毫秒后执行一个回调，同时又要求在200毫秒后执行另外一个回调，这就要求事件循环在它的每一轮执行之前，首先要找出最近需要执行的那次timer事件。这样事件循环在接下来等待中就知道该等待多长时间（在这个例子中，我们需要等待100毫秒）。 等待事件发生。这一步我们需要能够同时等待timer和I/O两种事件的发生。要做到这一点，我们依赖系统低层的I/O多路复用机制。这种机制一般是这样设计的：它允许我们针对多个文件描述符来等待对应的I/O事件发生，并同时可以指定一个最长的阻塞超时时间。如果在这段阻塞时间内，有I/O事件发生，那么程序会被唤醒继续执行；如果一直没有I/O事件发生，而是指定的时间先超时了，那么程序也会被唤醒。对于timer事件的等待，就是依赖这里的超时机制。当然，这里的超时时间也可以指定成无限长，这就相当于只等待I/O事件。我们再看一下上一步查找最近timer事件，查找完之后可能有三种结果，因此这一步等待也可能出现三种对应的情况： 第一种情况，查找到了一个最近的timer事件，它要求在未来某一个时刻触发。那么，这一步只需要把这个未来时刻转换成阻塞超时时间即可。 第二种情况，查找到了一个最近的timer事件，但它要求的时刻已经过去了。那么，这时候它应该立刻被触发，而不应该再有任何等待。当然，在实现的时候还是调用了事件等待的API，只是把超时事件设置成0就可以达到这个效果。 第三种情况，没有查找到任何注册的timer事件。那么，这时候应该把超时时间设置成无限长。接下来只有I/O事件发生才能唤醒。 判断有I/O事件发生还是超时。这里是程序从上一步（可能的）阻塞状态中恢复后执行逻辑。如果是I/O事件发生了，那么先指向I/O事件回调，然后根据需要把到期的timer事件的回调也执行掉（如果有的话）；如果是超时先发生了，那么表示只有timer事件需要触发（没有I/O事件发生），那么就直接把到期的timer事件的回调执行掉。 执行I/O事件回调。我们前面提到的对于TCP连接的监听和对于Unix domain socket的监听，这两种I/O事件的回调函数acceptTcpHandler和acceptUnixHandler，就是在这一步被调用的。 执行timer事件回调。我们前面提到的周期性回调函数serverCron，就是在这一步被调用的。一般情况下，一个timer事件被处理后，它就会被从队列中删除，不会再次执行了。但serverCron却是被周期性调用的，这是怎么回事呢？这是因为Redis对于timer事件回调的处理设计了一个小机制：timer事件的回调函数可以返回一个需要下次执行的毫秒数。如果返回值是正常的正值，那么Redis就不会把这个timer事件从事件循环的队列中删除，这样它后面还有机会再次执行。例如，按照默认的设置，serverCron返回值是100，因此它每隔100毫秒会执行一次（当然这个执行频率可以在redis.conf中通过hz变量来调整）。 至此，Redis整个事件循环的轮廓我们就清楚了。Redis主要的处理流程，包括接收请求、执行命令，以及周期性地执行后台任务（serverCron），都是由这个事件循环驱动的。当请求到来时，I/O事件被触发，事件循环被唤醒，根据请求执行命令并返回响应结果；同时，后台异步任务（如回收过期的key）被拆分成若干小段，由timer事件所触发，夹杂在I/O事件处理的间隙来周期性地运行。这种执行方式允许仅仅使用一个线程来处理大量的请求，并能提供快速的响应时间。当然，这种实现方式之所以能够高效运转，除了事件循环的结构之外，还得益于系统提供的异步的I/O多路复用机制(I/O multiplexing)。事件循环使得CPU资源被分时复用了，不同代码块之间并没有「真正的」并发执行，但I/O多路复用机制使得CPU和I/O的执行是真正并发的。而且，使用单线程还有额外的好处：避免了代码的并发执行，在访问各种数据结构的时候都无需考虑线程安全问题，从而大大降低了实现的复杂度。 Redis命令请求的处理流程概述我们在前面讨论[注册I/O事件回调]的时候提到过，Redis对于来自客户端的请求的处理，都会走到acceptTcpHandler和acceptUnixHandler这两个回调函数中去。实际上，这样的描述还过于粗略。 Redis客户端向服务器发送命令，可以细分为两个过程： 1.建立连接。客户端发起连接请求（通过TCP或Unix Domain Socket）,服务器接受连接。 2.命令发送、执行和响应。连接一旦建立好，客户端就可以在这个新连接的基础上发送命令数据，服务器收到后执行这个命令，并把执行结果返回给客户端。而且，在新连接上，这整个的[命令发送、执行和响应]的过程就可以反复执行。 上述第一个过程，「连接建立」，对应到服务端的代码，就是会走到acceptTcpHandler或acceptUnixHandler这两个回调函数中去。换句话说，Redis服务器每收到一个新的连接请求，就会由事件循环触发一个I/O事件，从而执行到acceptTcpHandler或acceptUnixHandler回调函数的代码。 接下来，从socket编程的角度，服务器应该调用accept系统API来接受连接请求，并为新的连接创建出一个socket。这个新的socket也就对应着一个新的文件描述符。为了在新的连接上能接收到客户端发来的命令，接下来必须在事件循环中为这个新的文件描述符注册一个I/O事件回调。这个过程的流程图如下： 图2：接收客户端连接请求 从上面流程图可以看出，新的连接注册了一个I/O事件回调，即readQueryFromClient。也就是说，对应前面讲的第二个过程，[命令发送、执行和响应]，当服务器收到命令数据的时候，也会由事件循环触发一个I/O事件，执行到readQueryFromClient回调。这个函数的实现就是在处理命令的[执行和响应]了。因此，下面我们看一下这个函数的执行流程图。 图3：readQueryFromClient函数执行流程图 从socket中读入数据，是按照流的方式。也就是说，站在应用层的角度，从底层网络层读入的数据，是由一个个字节组成的字节流。而我们需要从这些字节流中解析出完整的Redis命令，才能知道接下来如何处理。但由于网络传输的特点，我们并不能控制一次读入多少个字节。实际上，即使服务器只是收到一个Redis命令的部分数据（哪怕只有一个字节），也有可能触发一次I/O事件回调。这时我们是调用read系统API来读入数据的。虽然调用read时我们可以指定期望读取的字节数，但它并不会保证一定能返回期望长度的数据。比如我们想读100个字节，但可能只能读到80个字节，剩下的20个字节可能还在网络传输中没有到达。这种情况给接收Redis命令的过程造成了很大的麻烦：首先，可能我们读到的数据还不够一个完整的命令，这时我们应该继续等待更多的数据到达。其次，我们可能一次性收到了大量的数据，里面包含不止一个命令，这时我们必须把里面包含的所有命令都解析出来，而且要正确解析到最后一个完整命令的边界。如果最后一个完整命令后面还有多余的数据，那么这些数据应该留在下次有更多数据到达时再处理。这个复杂的过程一般称为「粘包」。 「粘包」处理的第一个表现，就是当尝试解析出一个完整的命令时，如果解析失败了，那么上面的流程就直接退出了。接下来，如果有更多数据到达，事件循环会再次触发I/O事件回调，重新进入上面的流程继续处理。 「粘包」处理的第二个表现，是上面流程图中的大循环。只要暂存输入数据的query buffer中还有数据可以处理，那么就不停地去尝试解析完整命令，直到把里面所有的完整命令都处理完，才退出循环。 查命令表那一步，就是查找本文前面提到的由populateCommandTable初始化的命令表，这个命令表存储在server.c的全局变量redisCommandTable当中。命令表中存有各个Redis命令的执行入口。 对于命令的执行结果，在上面的流程图中只是最后存到了一个输出buffer中，并没有真正输出给客户端。输出给客户端的过程不在这个流程当中，而是由另外一个同样是由事件循环驱动的过程来完成。这个过程涉及很多细节，我们在这里先略过，留在后面第四部分再来讨论。 事件机制介绍在本文第一部分，我们提到过，我们必须有一种机制能够同时等待I/O和timer这两种事件的发生。这一机制就是系统底层的I/O多路复用机制(I/O multiplexing)。但是，在不同的系统上，存在多种不同的I/O多路复用机制。因此，为了方便上层程序实现，Redis实现了一个简单的事件驱动程序库，即ae.c的代码，它屏蔽了系统底层在事件处理上的差异，并实现了我们前面一直在讨论的事件循环。 在Redis的事件库的实现中，目前它低层支持4种I/O多路复用机制： select系统调用，这应该是最早出现的一种I/O多路复用机制了，于1983年在4.2BSD Unix中被首次使用。它是POSIX规范的一部分。另外，跟select类似的还有一个poll系统调用，它是1986年在SVR3 Unix系统中首次使用的，也遵循POSIX规范。只要是遵循POSIX规范的操作系统，它就能支持select和poll机制，因此在目前我们常见的系统中这两种I/O事件机制一般都是支持的。 epoll机制。epoll是比select更新的一种I/O多路复用机制，最早出现在Linux内核的2.5.44版本中。它被设计出来是为了代替旧的select和poll，提供一种更高效的I/O机制。注意，epoll是Linux系统所特有的，它不属于POSIX规范。 kqueue机制。kqueue最早是2000年在FreeBSD 4.1上被设计出来的，后来也支持NetBSD、OpenBSD、DragonflyBSD和macOS系统。它和Linux系统上的epoll是类似的。 event ports。这是在illumos系统上特有的一种I/O事件机制。 既然在不同系统上有不同的事件机制，那么Redis在不同系统上编译时采用的是哪个机制呢？由于在上面四种机制中，后三种是更现代，也是比select和poll更高效的方案，因此Redis优先选择使用后三种机制。 通过上面对各种I/O机制所适用的操作系统的总结，我们很容易看出，如果你在macOS上编译Redis，那么它底层会选用kqueue；而如果在Linux上编译则会选择epoll，这也是Redis在实际运行中比较常见的情况。 现在我们回过头来再看一下底层的这些I/O事件机制是如何支持了Redis的事件循环的（下面的描述是对本文前面第一部分中事件循环流程的细化）： 首先，向事件循环中注册I/O事件回调的时候，需要指定哪个回调函数注册到哪个事件上（事件用文件描述符来表示）。事件和回调函数的对应关系，由Redis上层封装的事件驱动程序库来维护。具体参见函数aeCreateFileEvent的代码。 类似地，向事件循环中注册timer事件回调的时候，需要指定多长时间之后执行哪个回调函数。这里需要记录哪个回调函数预期在哪个时刻被调用，这也是由Redis上层封装的事件驱动程序库来维护的。具体参见函数aeCreateTimeEvent的代码。 底层的各种事件机制都会提供一个等待事件的操作，比如epoll提供的epoll_wait API。这个等待操作一般可以指定预期等待的事件列表（事件用文件描述符来表示），并同时可以指定一个超时时间（即最大等待多长时间）。在事件循环中需要等待事件发生的时候，就调用这个等待操作，传入之前注册过的所有I/O事件，并把最近的timer事件所对应的时刻转换成这里需要的超时时间。具体参见函数aeProcessEvents的代码。 从上一步的等待操作中唤醒，有两种情况：如果是I/O事件发生了，那么就根据触发的事件查到I/O回调函数，进行调用；如果是超时了，那么检查所有注册过的timer事件，对于预期调用时刻超过当前时间的回调函数都进行调用。 最后，关于事件机制，还有一些信息值得关注：业界已经有一些比较成熟的开源的事件库了，典型的比如libevent和libev。一般来说，这些开源库屏蔽了非常复杂的底层系统细节，并对不同的系统版本实现做了兼容，是非常有价值的。那为什么Redis的作者还是自己实现了一套呢？在Google Group的一个帖子上，Redis的作者给出了一些原因。帖子地址如下： https://groups.google.com/group/redis-db/browse_thread/thread/b52814e9ef15b8d0/ 原因大致总结起来就是： 不想引入太大的外部依赖。比如libevent太大了，比Redis的代码库还大。 方便做一些定制化的开发。 第三方库有时候会出现一些意想不到的bug。]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CSRF攻击与防御]]></title>
    <url>%2F2019%2F04%2F02%2FCSRF%E6%94%BB%E5%87%BB%E4%B8%8E%E9%98%B2%E5%BE%A1%2F</url>
    <content type="text"><![CDATA[转载至 http://www.cnblogs.com/hyddd/archive/2009/04/09/1432744.html 一、CSRF是什么？CSRF（Cross-site request forgery），中文名称：跨站请求伪造，也被称为：one click attack/session riding，缩写为：CSRF/XSRF。 二、CSRF可以做什么？你这可以这么理解CSRF攻击：攻击者盗用了你的身份，以你的名义发送恶意请求。CSRF能够做的事情包括：以你名义发送邮件，发消息，盗取你的账号，甚至于购买商品，虚拟货币转账……造成的问题包括：个人隐私泄露以及财产安全。 三、CSRF漏洞现状CSRF这种攻击方式在2000年已经被国外的安全人员提出，但在国内，直到06年才开始被关注，08年，国内外的多个大型社区和交互网站分别爆出CSRF漏洞，如：NYTimes.com（纽约时报）、Metafilter（一个大型的BLOG网站），YouTube和百度HI……而现在，互联网上的许多站点仍对此毫无防备，以至于安全业界称CSRF为“沉睡的巨人”。 四、CSRF的原理下图简单阐述了CSRF攻击的思想： 图1：CSRF攻击原理 从上图可以看出，要完成一次CSRF攻击，受害者必须依次完成两个步骤： 1.登录受信任网站A，并在本地生成Cookie。 2.在不登出A的情况下，访问危险网站B。 看到这里，你也许会说：“如果我不满足以上两个条件中的一个，我就不会受到CSRF的攻击”。是的，确实如此，但你不能保证以下情况不会发生： 1.你不能保证你登录了一个网站后，不再打开一个tab页面并访问另外的网站。 2.你不能保证你关闭浏览器了后，你本地的Cookie立刻过期，你上次的会话已经结束。（事实上，关闭浏览器不能结束一个会话，但大多数人都会错误的认为关闭浏览器就等于退出登录/结束会话了……） 3.上图中所谓的攻击网站，可能是一个存在其他漏洞的可信任的经常被人访问的网站。 上面大概地讲了一下CSRF攻击的思想，下面我将用几个例子详细说说具体的CSRF攻击，这里我以一个银行转账的操作作为例子（仅仅是例子，真实的银行网站没这么傻:&gt;） 示例1银行网站A，它以GET请求来完成银行转账的操作，如：http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000 危险网站B，它里面有一段HTML的代码如下：1&lt;img src=http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000&gt; 首先，你登录了银行网站A，然后访问危险网站B，噢，这时你会发现你的银行账户少了1000块…… 为什么会这样呢？原因是银行网站A违反了HTTP规范，使用GET请求更新资源。在访问危险网站B的之前，你已经登录了银行网站A，而B中的以GET的方式请求第三方资源（这里的第三方就是指银行网站了，原本这是一个合法的请求，但这里被不法分子利用了），所以你的浏览器会带上你的银行网站A的Cookie发出Get请求，去获取资源”http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000&quot;，结果银行网站服务器收到请求后，认为这是一个更新资源操作（转账操作），所以就立刻进行转账操作...... 示例2为了杜绝上面的问题，银行决定改用POST请求完成转账操作。 银行网站A的WEB表单如下： 12345&lt;form action="Transfer.php" method="POST"&gt; &lt;p&gt;ToBankId: &lt;input type="text" name="toBankId" /&gt;&lt;/p&gt; &lt;p&gt;Money: &lt;input type="text" name="money" /&gt;&lt;/p&gt; &lt;p&gt;&lt;input type="submit" value="Transfer" /&gt;&lt;/p&gt;&lt;/form&gt; 后台处理页面Transfer.php如下： 1234567&lt;?php session_start(); if (isset($_REQUEST['toBankId'] &amp;&amp; isset($_REQUEST['money'])) &#123; buy_stocks($_REQUEST['toBankId'], $_REQUEST['money']); &#125;?&gt; 危险网站B，仍然只是包含那句HTML代码： 1&lt;img src=http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000&gt; 和示例1中的操作一样，你首先登录了银行网站A，然后访问危险网站B，结果…..和示例1一样，你再次没了1000块～T_T，这次事故的原因是：银行后台使用了$_REQUEST去获取请求的数据，而$_REQUEST既可以获取GET请求的数据，也可以获取POST请求的数据，这就造成了在后台处理程序无法区分这到底是GET请求的数据还是POST请求的数据。在PHP中，可以使用$_GET和$_POST分别获取GET请求和POST请求的数据。在JAVA中，用于获取请求数据request一样存在不能区分GET请求数据和POST数据的问题。 示例3： 经过前面2个惨痛的教训，银行决定把获取请求数据的方法也改了，改用$_POST，只获取POST请求的数据，后台处理页面Transfer.php代码如下： 1234567&lt;?php session_start(); if (isset($_POST['toBankId'] &amp;&amp; isset($_POST['money'])) &#123; buy_stocks($_POST['toBankId'], $_POST['money']); &#125;?&gt; 然而，危险网站B与时俱进，它改了一下代码： 1234567891011121314151617181920&lt;html&gt; &lt;head&gt; &lt;script type="text/javascript"&gt; function steal() &#123; iframe = document.frames["steal"]; iframe.document.Submit("transfer"); &#125; &lt;/script&gt; &lt;/head&gt; &lt;body onload="steal()"&gt; &lt;iframe name="steal" display="none"&gt; &lt;form method="POST" name="transfer" action="http://www.myBank.com/Transfer.php"&gt; &lt;input type="hidden" name="toBankId" value="11"&gt; &lt;input type="hidden" name="money" value="1000"&gt; &lt;/form&gt; &lt;/iframe&gt; &lt;/body&gt;&lt;/html&gt; 如果用户仍是继续上面的操作，很不幸，结果将会是再次不见1000块……因为这里危险网站B暗地里发送了POST请求到银行! 总结一下上面3个例子，CSRF主要的攻击模式基本上是以上的3种，其中以第1,2种最为严重，因为触发条件很简单，一个就可以了，而第3种比较麻烦，需要使用JavaScript，所以使用的机会会比前面的少很多，但无论是哪种情况，只要触发了CSRF攻击，后果都有可能很严重。 理解上面的3种攻击模式，其实可以看出，CSRF攻击是源于WEB的隐式身份验证机制！WEB的身份验证机制虽然可以保证一个请求是来自于某个用户的浏览器，但却无法保证该请求是用户批准发送的！ 五.CSRF的防御我总结了一下看到的资料，CSRF的防御可以从服务端和客户端两方面着手，防御效果是从服务端着手效果比较好，现在一般的CSRF防御也都在服务端进行。 1.服务器端进行CSRF防御服务端的CSRF方式方法很多样，但总的思想都是一致的，就是在客户端页面增加伪随机数。 1).Cookie Hashing(所有表单都包含同一个伪随机值)：这可能是最简单的解决方案了，因为攻击者不能获得第三方的Cookie(理论上)，所以表单中的数据也就构造失败了:&gt; 12345&lt;?php //构造加密的Cookie信息 $value = “DefenseSCRF”; setcookie(”cookie”, $value, time()+3600);?&gt; 在表单里增加Hash值，以认证这确实是用户发送的请求。 123456789&lt;?php $hash = md5($_COOKIE['cookie']);?&gt; &lt;form method=”POST” action=”transfer.php”&gt; &lt;input type=”text” name=”toBankId”&gt; &lt;input type=”text” name=”money”&gt; &lt;input type=”hidden” name=”hash” value=”&lt;?=$hash;?&gt;”&gt; &lt;input type=”submit” name=”submit” value=”Submit”&gt; &lt;/form&gt; 然后再服务器端进行Hash值验证 12345678910111213&lt;?php if(isset($_POST['check'])) &#123; $hash = md5($_COOKIE['cookie']); if($_POST['check'] == $hash) &#123; doJob(); &#125; else &#123; //... &#125; else &#123; //... &#125;?&gt; 这个方法个人觉得已经可以杜绝99%的CSRF攻击了，那还有1%呢….由于用户的Cookie很容易由于网站的XSS漏洞而被盗取，这就另外的1%。一般的攻击者看到有需要算Hash值，基本都会放弃了，某些除外，所以如果需要100%的杜绝，这个不是最好的方法。 2).验证码这个方案的思路是：每次的用户提交都需要用户在表单中填写一个图片上的随机字符串，厄….这个方案可以完全解决CSRF，但个人觉得在易用性方面似乎不是太好，还有听闻是验证码图片的使用涉及了一个被称为MHTML的Bug，可能在某些版本的微软IE中受影响。 3).One-Time Tokens(不同的表单包含一个不同的伪随机值)在实现One-Time Tokens时，需要注意一点：就是“并行会话的兼容”。如果用户在一个站点上同时打开了两个不同的表单，CSRF保护措施不应该影响到他对任何表单的提交。考虑一下如果每次表单被装入时站点生成一个伪随机值来覆盖以前的伪随机值将会发生什么情况：用户只能成功地提交他最后打开的表单，因为所有其他的表单都含有非法的伪随机值。必须小心操作以确保CSRF保护措施不会影响选项卡式的浏览或者利用多个浏览器窗口浏览一个站点。 4)slim框架CSRF实现slim框架的csrf预防源码]]></content>
      <tags>
        <tag>web安全</tag>
        <tag>CSRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[源码文件]]></title>
    <url>%2F2019%2F03%2F29%2F%E6%BA%90%E7%A0%81%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[go文件主要分为下面3类: 图1：go文件]]></content>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工作区和GOPATH]]></title>
    <url>%2F2019%2F03%2F29%2F%E5%B7%A5%E4%BD%9C%E5%8C%BA%E5%92%8CGOPATH%2F</url>
    <content type="text"><![CDATA[我们学习Go语言时，第一件要做的是，就是根据自己电脑的操作系统和计算架构，从Go语言官网下载对应的二进制包，也就是拿来即用的安装包。 随后，解压安装包、放置到某个目录、配置环境变量，并在命令行输入 go version 来验证是否安装成功。 1234567891011121314151617# 解压[root@www package]# ls go1.12.1.linux-amd64.tar.gz go1.12.1.linux-amd64.tar.gz[root@www package]# tar zxvf go1.12.1.linux-amd64.tar.gz -C /usr/local/# 设置GOROOT、GOPATH、GOBIN环境变量[root@www ~]# vim .bash_profilePATH=$PATH:$HOME/bin:/usr/local/php/bin:/usr/local/go/binexport PATHexport GOROOT=/usr/local/goexport GOPATH=/root/peek-a-bowexport GOBIN=/root/peek-a-bow/bin# 创建工作目录mkdir -p /root/peek-a-bow/&#123;src,bin,pkg&#125; 在整个安装过程中，需要配置3个环境变量，简单介绍一下: GOROOT: Go语言的安装目录 GOPATH: 自定义的工作目录 GOBIN: Go语言生成的可执行文件的目录 可以把GOPATH简单理解成Go语言的工作目录，它的值是一个目录的路径，也可以是多个目录的路径，每个目录都代表Go语言的一个工作区（workspace）。 我们需要利于这些工作区，去放置 Go 语言的源码文件（source file）,以及安装(install)后的归档文件（archive file）和可执行文件（executable file）。 事实上，由于Go语言项目在其生命周期内的所有操作（编码依赖管理、构建、测试、安装等）基本上都是围绕着GOPATH和工作区进行的。它的背后有3个知识点需要注意: 1.Go语言源码的组织是怎样的； 2.你是否了解源码安装后的结果； 3.你是否理解构建和安装Go程序的过程。 1.Go语言源码组织方式Go语言是以代码包为基本组织单位的。所以说，Go语言源码的组织方式就是以环境变量GOPATH、工作区、src目录和代码包为主线的。 2.了解源码安装后的结果源码文件在安装过程中，如果产生了归档文件，就会放进该工作区的pkg子目录；如果产生了可执行文件，就会放进该工作区的bin子目录。 3.理解构建和安装Go程序的过程构建使用 go build,安装使用命令 go install。构建和安装代码包的时候都会执行编译、打包等操作。并且，这些操作产生的任何文件都会先被保存到某个临时的目录中。 如果构建的是库源码文件,那么操作结果只会保存在临时目录中，安装库源码文件，那么它的结果会被搬运到它所在工作区的pkg目录下的某个子目录中。 如果构建的是命令源码文件，那么它的操作结果文件会被搬运到源码文件所在的目录中。如果安装的是命令源码文件，那么结果文件会被搬运到它所在工作区的bin目录中，或者环境变量GOBIN指向的目录中。]]></content>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go string 实现原理剖析]]></title>
    <url>%2F2019%2F03%2F28%2FGo-string-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[倒排索引原理]]></title>
    <url>%2F2019%2F03%2F27%2F%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[关于ElasticSearch为什么搜索这么快，大家应该有所了解，主要是利用倒排索引数据结构，下面简单介绍一下倒排索引。 正向索引任何事物都是相对的，有倒排索引(inverted index)，当然也会有正向索引(forward index)。 正向索引结构在搜索引擎中，每个文件(document)对应一个文件id(document id)，文件内容可以看作是一些列关键词的集合(实际上，在搜索引擎库中，关键词也转化为关键词id)。例如“文档1”经过分词，提取了20个关键词，每个关键词都会记录它在文档中的出现次数和出现位置。 得到正向索引结构如下： “文档1”的ID &gt; 关键词1：出现次数，出现位置列表；关键词2：出现次数，出现位置列表；…… 图1：正向索引结构 通过文档，去找关键词。 正向索引查找过程当用户在主页上搜索关键词“华为手机”时，假设只存在正向索引(forward index)，那么就需要扫描索引库中的所有文档，找出所有包含关键词“华为手机”的文档，再根据打分模型进行打分，排出名次后呈现给用户。因为互联网上收录在搜索引擎中的文档的数目是个天文数字，这样的索引结构根本无法满足实时返回排名结果的要求。 倒排索引倒排索引结构由于正向索引无法满足实时返回排名结果的要求，所以，搜索引擎会将正向索引重新构建为倒排索引，即把文件id对应到关键词的映射转化为关键词到文件id的映射，每个关键词对应着一些列文件，这些文件中都出现这个关键词。 得到倒排索引的结构如下： “关键词1” ：“文档1”的id ， “文档2”的id ，……。 图2：倒排索引结构 通过关键词，去找文档。 单词-文档矩阵单词-文档矩阵是表达两者之间所具有的一种包含关系的概念模型。 图3：单词-文档矩阵 从纵向看（即从文档这个维度看），每列代表文档包含了哪些单词，比如文档1包含了词汇1和词汇4，而不包含其它单词。 从横向看（即从单词这个维度看），每行代表了哪些文档包含了这个单词。比如词汇1来说，文档1和文档4中出现过单词1，而其它文档不包含词汇1。 搜索引擎，其实就是实现了 “单词-文档矩阵”的具体数据结构，可以有不同的方式来实现上述概念模型，比如“倒排索引”、“签名文件”、“后缀树”等方式。目前ElasticSearch中是使用“倒排索引”实现单词到文档映射关系。 倒排索引基本概念文档(document):一般搜索引擎的处理对象是互联网网页，而文档这个概念要更宽泛些，代表以文本形式存在的存储对象，相比网页来说，涵盖更多种形式，比如Word，PDF，html，XML等不同格式的文件都可以称之为文档。再比如一封邮件，一条短信，一条微博也可以称之为文档。在本书后续内容，很多情况下会使用文档来表征文本信息。 文档集合(Document Collection)：由若干文档构成的集合称之为文档集合。比如海量的互联网网页或者说大量的电子邮件都是文档集合的具体例子。 文档编号(Document ID):在搜索引擎内部，会将文档集合内每个文档赋予一个唯一的内部编号，以此编号来作为这个文档的唯一标识，这样方便内部处理，每个文档的内部编号即称之为“文档编号”，后文有时会用DocID来便捷地代表文档编号。 单词编号(Word ID):与文档编号类似，搜索引擎内部以唯一的编号来表征某个单词，单词编号可以作为某个单词的唯一表征。 倒排索引(Inverted Index):倒排索引是实现“单词-文档矩阵”的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。 单词字典(Lexicon)：搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。 倒排列表(PostingList)：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。 倒排文件(Inverted File)：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件即被称之为倒排文件，倒排文件是存储倒排索引的物理文件。 关于这些概念之间的关系，通过图4可以比较清晰的看出来。 图4：倒排列表模型 倒排索引简单实例倒排索引从逻辑结构和基本思路上来讲非常简单。下面我们通过具体实例来进行说明，使得读者能够对倒排索引有一个宏观而直接的感受。 假设文档集合包含五个文档，每个文档内容如图3所示，在图中最左端一栏是每个文档对应的文档编号。我们的任务就是对这个文档集合建立倒排索引。 图5：文档集合 中文和英文等语言不同，单词之间没有明确分隔符号，所以首先要用分词系统将文档自动切分成单词序列。这样每个文档就转换为由单词序列构成的数据流，为了系统后续处理方便，需要对每个不同的单词赋予唯一的单词编号，同时记录下哪些文档包含这个单词，在如此处理结束后，我们可以得到最简单的倒排索引（参考图6）。在图6中，“单词ID”一栏记录了每个单词的单词编号，第二栏是对应的单词，第三栏即每个单词对应的倒排列表。比如单词“谷歌”，其单词编号为1，倒排列表为{1,2,3,4,5}，说明文档集合中每个文档都包含了这个单词。 图6：最简单的倒排索引 之所以说图6所示倒排索引是最简单的，是因为这个索引系统只记载了哪些文档包含某个单词，而事实上，索引系统还可以记录除此之外的更多信息。图7是一个相对复杂些的倒排索引，与图6的基本索引系统比，在单词对应的倒排列表中不仅记录了文档编号，还记载了单词频率信息（TF），即这个单词在某个文档中的出现次数，之所以要记录这个信息，是因为词频信息在搜索结果排序时，计算查询和文档相似度是很重要的一个计算因子，所以将其记录在倒排列表中，以方便后续排序时进行分值计算。在图7的例子里，单词“创始人”的单词编号为7，对应的倒排列表内容为：（3:1），其中的3代表文档编号为3的文档包含这个单词，数字1代表词频信息，即这个单词在3号文档中只出现过1次，其它单词对应的倒排列表所代表含义与此相同。 图7：带有单词频率信息的倒排索引 实用的倒排索引还可以记载更多的信息，图8所示索引系统除了记录文档编号和单词频率信息外，额外记载了两类信息，即每个单词对应的“文档频率信息”（对应图8的第三栏）以及在倒排列表中记录单词在某个文档出现的位置信息。 “文档频率信息”代表了在文档集合中有多少个文档包含某个单词，之所以要记录这个信息，其原因与单词频率信息一样，这个信息在搜索结果排序计算中是非常重要的一个因子。而单词在某个文档中出现的位置信息并非索引系统一定要记录的，在实际的索引系统里可以包含，也可以选择不包含这个信息，之所以如此，因为这个信息对于搜索系统来说并非必需的，位置信息只有在支持“短语查询”的时候才能够派上用场。 以单词“拉斯”为例，其单词编号为8，文档频率为2，代表整个文档集合中有两个文档包含这个单词，对应的倒排列表为：{(3;1;)，(5;1;)},其含义为在文档3和文档5出现过这个单词，单词频率都为1，单词“拉斯”在两个文档中的出现位置都是4，即文档中第四个单词是“拉斯”。 图7所示倒排索引已经是一个非常完备的索引系统，实际搜索系统的索引结构基本如此，区别无非是采取哪些具体的数据结构来实现上述逻辑结构。 有了这个索引系统，搜索引擎可以很方便地响应用户的查询，比如用户输入查询词“Facebook”，搜索系统查找倒排索引，从中可以读出包含这个单词的文档，这些文档就是提供给用户的搜索结果，而利用单词频率信息、文档频率信息即可以对这些候选搜索结果进行排序，计算文档和查询的相似性，按照相似性得分由高到低排序输出，此即为搜索系统的部分内部流程。 单词词典单词词典是倒排索引中非常重要的组成部分，它用来维护文档集合中出现过的所有单词的相关信息，同时用来记载某个单词对应的倒排列表在倒排文件中的位置信息。在支持搜索时，根据用户的查询词，去单词词典里查询，就能够获得相应的倒排列表，并以此作为后续排序的基础。 对于一个规模很大的文档集合来说，可能包含几十万甚至上百万的不同单词，能否快速定位某个单词，这直接影响搜索时的响应速度，所以需要高效的数据结构来对单词词典进行构建和查找，常用的数据结构包括哈希加链表结构和树形词典结构。 哈希加链表图8是这种词典结构的示意图。这种词典结构主要由两个部分构成： 主体部分是哈希表，每个哈希表项保存一个指针，指针指向冲突链表，在冲突链表里，相同哈希值的单词形成链表结构。之所以会有冲突链表，是因为两个不同单词获得相同的哈希值，如果是这样，在哈希方法里被称做是一次冲突，可以将相同哈希值的单词存储在链表里，以供后续查找。 图8：哈希加链表结构 在建立索引的过程中，词典结构也会相应地被构建出来。比如在解析一个新文档的时候，对于某个在文档中出现的单词T，首先利用哈希函数获得其哈希值，之后根据哈希值对应的哈希表项读取其中保存的指针，就找到了对应的冲突链表。如果冲突链表里已经存在这个单词，说明单词在之前解析的文档里已经出现过。如果在冲突链表里没有发现这个单词，说明该单词是首次碰到，则将其加入冲突链表里。通过这种方式，当文档集合内所有文档解析完毕时，相应的词典结构也就建立起来了。 在响应用户查询请求时，其过程与建立词典类似，不同点在于即使词典里没出现过某个单词，也不会添加到词典内。以图8为例，假设用户输入的查询请求为单词3，对这个单词进行哈希，定位到哈希表内的2号槽，从其保留的指针可以获得冲突链表，依次将单词3和冲突链表内的单词比较，发现单词3在冲突链表内，于是找到这个单词，之后可以读出这个单词对应的倒排列表来进行后续的工作，如果没有找到这个单词，说明文档集合内没有任何文档包含单词，则搜索结果为空。 树形结构 B树（或者B+树）是另外一种高效查找结构。B树与哈希方式查找不同，需要字典项能够按照大小排序（数字或者字符序），而哈希方式则无须数据满足此项要求。 B树形成了层级查找结构，中间节点用于指出一定顺序范围的词典项目存储在哪个子树中，起到根据词典项比较大小进行导航的作用，最底层的叶子节点存储单词的地址信息，根据这个地址就可以提取出单词字符串。 总结图9：文档集合 图10：倒排索引结构 单词ID：记录每个单词的单词编号； 单词：对应的单词； 文档频率：代表文档集合中有多少个文档包含某个单词 倒排列表：包含单词ID及其他必要信息 DocId：单词出现的文档id TF：单词在某个文档中出现的次数 POS：单词在文档中出现的位置 以单词“加盟”为例，其单词编号为6，文档频率为3，代表整个文档集合中有三个文档包含这个单词，对应的倒排列表为{(2;1;),(3;1;),(5;1;)}，含义是在文档2，3，5出现过这个单词，在每个文档的出现过1次，单词“加盟”在第一个文档的POS是4，即文档的第四个单词是“加盟”，其他的类似。 这个倒排索引已经是一个非常完备的索引系统，实际搜索系统的索引结构基本如此。]]></content>
      <tags>
        <tag>倒排索引</tag>
      </tags>
  </entry>
</search>
